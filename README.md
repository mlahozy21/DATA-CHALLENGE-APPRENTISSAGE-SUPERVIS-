# Data Challenge - Apprentissage SupervisÃ© 

Projet rÃ©alisÃ© dans le cadre du cours **Apprentissage SupervisÃ© AvancÃ©** du Master M2 MathÃ©matiques et Intelligence Artificielle Ã  l'UniversitÃ© Paris-Saclay (2025).

## Table des matiÃ¨res

- [Challenge 1 : Classification des rÃ©servations hÃ´teliÃ¨res](#challenge-1--classification-des-rÃ©servations-hÃ´teliÃ¨res)
- [Challenge 2 : RÃ©gression de la popularitÃ© Spotify](#challenge-2--rÃ©gression-de-la-popularitÃ©-spotify)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [Auteurs](#auteurs)

## Challenge 1 : Classification des rÃ©servations hÃ´teliÃ¨res

### ProblÃ©matique

Dans le secteur hÃ´telier, les annulations et les no-show entraÃ®nent des pertes financiÃ¨res importantes. L'objectif est de prÃ©dire le statut final d'une rÃ©servation parmi 3 catÃ©gories :

- **0** : Check-out (client s'est prÃ©sentÃ©)
- **1** : Cancel (rÃ©servation annulÃ©e)
- **2** : No-Show (client ne s'est pas prÃ©sentÃ©)


## ðŸŽµ Challenge 2 : RÃ©gression de la popularitÃ© Spotify

### ProblÃ©matique

L'industrie musicale cherche Ã  comprendre les facteurs de popularitÃ© d'un titre. L'objectif est de prÃ©dire la popularitÃ© Spotify (0-100) Ã  partir des caractÃ©ristiques audio et mÃ©tadonnÃ©es.

## ðŸ”§ Installation

### PrÃ©requis

- Python 3.8+
- pip ou conda

### Installation des dÃ©pendances

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/mlahozy21/DATA-CHALLENGE-APPRENTISSAGE-SUPERVIS-.git
cd DATA-CHALLENGE-APPRENTISSAGE-SUPERVIS-

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### DÃ©pendances principales

```
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
lightgbm>=3.3.0
catboost>=1.0.0
joblib>=1.1.0
```

## Utilisation

### Challenge 1 : Classification

```bash
cd classification/

# 1. EntraÃ®ner les modÃ¨les de base (L0)
python train_base_models.py

# 2. EntraÃ®ner le mÃ©ta-modÃ¨le (L1)
python train_meta_model.py

# 3. GÃ©nÃ©rer les prÃ©dictions finales
python predict_stacking.py
```

**Sortie** : `submission.csv` contenant les prÃ©dictions pour l'ensemble de test

### Challenge 2 : RÃ©gression

```bash
cd regression/

# 1. EntraÃ®ner les modÃ¨les de base (L0)
python train_base_models.py

# 2. EntraÃ®ner le mÃ©ta-modÃ¨le (L1)
python train_meta_model.py

# 3. GÃ©nÃ©rer les prÃ©dictions finales
python predict_stacking.py
```

**Sortie** : `submission.csv` contenant les prÃ©dictions de popularitÃ©

### Configuration personnalisÃ©e

Les hyperparamÃ¨tres et chemins peuvent Ãªtre modifiÃ©s dans `config.py` :

```python
# Exemple : modifier le learning rate de LightGBM
LGBM_PARAMS = {
    'learning_rate': 0.01,  # Modifier ici
    'n_estimators': 3000,
    'num_leaves': 35,
    # ...
}
```
## Structure du projet 

DATA-CHALLENGE-APPRENTISSAGE-SUPERVIS-/
â”‚
â”œâ”€â”€ classification/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ train_data.csv
â”‚   â”‚   â”œâ”€â”€ test_data.csv
â”‚   â”‚   â””â”€â”€ naive_submission.csv
â”‚   â”œâ”€â”€ models/                    # ModÃ¨les entraÃ®nÃ©s (crÃ©Ã© automatiquement)
â”‚   â”œâ”€â”€ processed/                 # DonnÃ©es prÃ©-traitÃ©es (crÃ©Ã© automatiquement)
â”‚   â”œâ”€â”€ train_base_models.py      # EntraÃ®nement des modÃ¨les L0
â”‚   â”œâ”€â”€ train_meta_model.py       # EntraÃ®nement du mÃ©ta-modÃ¨le L1
â”‚   â”œâ”€â”€ predict_stacking.py       # GÃ©nÃ©ration des prÃ©dictions finales
â”‚   â”œâ”€â”€ data_loader.py            # Chargement et nettoyage des donnÃ©es
â”‚   â”œâ”€â”€ feature_engineering.py    # CrÃ©ation de features
â”‚   â”œâ”€â”€ models.py                 # DÃ©finitions des modÃ¨les
â”‚   â””â”€â”€ config.py                 # Configuration et hyperparamÃ¨tres
â”‚
â”œâ”€â”€ regression/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ train_base_models.py
â”‚   â”œâ”€â”€ train_meta_model.py
â”‚   â”œâ”€â”€ predict_stacking.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ rapport.pdf                    # Rapport dÃ©taillÃ© (mÃ©thodologie + rÃ©sultats)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


## Auteurs

**Master 2 MathÃ©matiques et Intelligence Artificielle - UniversitÃ© Paris-Saclay**

- **Marcos Lahoz**
- **Judith Le Roy**
- **Bingjian Jiang**

**Cours** : Apprentissage SupervisÃ© et Data Challenge  
**Date** : Novembre 2025

## ðŸ“š RÃ©fÃ©rences

- Ke et al. (2017). *LightGBM: A Highly Efficient Gradient Boosting Decision Tree*. NeurIPS.
- Prokhorenkova et al. (2018). *CatBoost: unbiased boosting with categorical features*. NeurIPS.
- Wolpert (1992). *Stacked Generalization*. Neural Networks, 5(2):241-259.

## License

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---
